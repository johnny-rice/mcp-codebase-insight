name: Build Verification

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      config_file:
        description: 'Path to verification config file'
        required: false
        default: 'verification-config.json'
      min_coverage:
        description: 'Minimum test coverage percentage'
        required: false
        default: '80.0'
      max_failures:
        description: 'Maximum allowed test failures'
        required: false
        default: '0'
      python_version:
        description: 'Python version to use for verification'
        required: false
        default: '3.9'

jobs:
  verify:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
      fail-fast: false # Continue testing other Python versions even if one fails
    
    name: Verify with Python ${{ matrix.python-version }}
    environment:
      name: production
      url: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
    
    services:
      qdrant:
        image: qdrant/qdrant:latest
        ports:
          - 6333:6333
          - 6334:6334
        # Remove health check for now to let the container start properly
        # We'll check connectivity in a separate step
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch all history for dependencies analysis
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Wait for Qdrant and verify connection
      run: |
        echo "Waiting for Qdrant to start..."
        # Install curl if needed
        apt-get update && apt-get install -y curl || true
        
        # Wait for Qdrant to become available (retry for 30 seconds)
        retry_count=0
        max_retries=10
        until curl -s http://localhost:6333/collections || [ $retry_count -eq $max_retries ]
        do
          echo "Waiting for Qdrant... (attempt $retry_count of $max_retries)"
          sleep 3
          retry_count=$((retry_count+1))
        done
        
        if [ $retry_count -eq $max_retries ]; then
          echo "Qdrant service failed to become available after 30 seconds"
          exit 1
        fi
        
        echo "Qdrant service is accessible."
    
    - name: Setup private packages
      run: |
        # Create local-packages directory if it doesn't exist
        mkdir -p local-packages
        
        # If there are private packages in repositories, clone them here
        if [ ! -z "${{ secrets.PRIVATE_REPO_URL }}" ]; then
          echo "Setting up private package repository..."
          
          # Configure pip to use the private repository if provided
          mkdir -p ~/.pip
          echo "[global]" > ~/.pip/pip.conf
          echo "index-url = https://pypi.org/simple" >> ~/.pip/pip.conf
          
          # Add the private repository with token if available
          if [ ! -z "${{ secrets.PRIVATE_REPO_TOKEN }}" ]; then
            echo "extra-index-url = ${{ secrets.PRIVATE_REPO_URL }}:${{ secrets.PRIVATE_REPO_TOKEN }}@simple" >> ~/.pip/pip.conf
          else
            echo "extra-index-url = ${{ secrets.PRIVATE_REPO_URL }}/simple" >> ~/.pip/pip.conf
          fi
        fi
        
        # If there are local Git repositories for dependencies, clone them
        if [ ! -z "${{ secrets.MCP_SERVER_QDRANT_REPO }}" ]; then
          echo "Cloning mcp-server-qdrant from repository..."
          git clone ${{ secrets.MCP_SERVER_QDRANT_REPO }} local-packages/mcp-server-qdrant
          
          # Install the package in development mode
          cd local-packages/mcp-server-qdrant
          pip install -e .
          cd ../../
        fi
        
        # Similarly for uvx package if needed
        if [ ! -z "${{ secrets.UVX_REPO }}" ]; then
          echo "Cloning uvx from repository..."
          git clone ${{ secrets.UVX_REPO }} local-packages/uvx
          
          # Install the package in development mode
          cd local-packages/uvx
          pip install -e .
          cd ../../
        fi
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        
        # Make the requirements script executable
        chmod +x scripts/compile_requirements.sh
        
        # Set environment variables for private package handling
        export PRIVATE_REPO_URL="${{ secrets.PRIVATE_REPO_URL }}"
        export PRIVATE_REPO_TOKEN="${{ secrets.PRIVATE_REPO_TOKEN }}"
        export LOCAL_PACKAGE_PATHS="./local-packages"
        
        # Use the compile_requirements.sh script to generate version-specific requirements
        echo "Using compile_requirements.sh to generate dependencies for Python ${{ matrix.python-version }}..."
        # Set auto-yes for cleanup to avoid interactive prompts in CI
        echo "y" | ./scripts/compile_requirements.sh ${{ matrix.python-version }}
        
        # Install the generated requirements
        if [ -f requirements-${{ matrix.python-version }}.txt ]; then
          echo "Installing from version-specific requirements file..."
          pip install -r requirements-${{ matrix.python-version }}.txt
          
          # Install private packages if they're in a separate file
          if [ -f requirements-private-${{ matrix.python-version }}.txt ]; then
            echo "Installing private packages..."
            # Try to install private packages, but continue even if it fails
            pip install -r requirements-private-${{ matrix.python-version }}.txt || echo "Warning: Some private packages could not be installed"
          fi
        else
          echo "Version-specific requirements not found, falling back to standard requirements.txt"
          pip install -r requirements.txt || {
            echo "Error installing from requirements.txt, attempting to fix compatibility issues..."
            grep -v "^#" requirements.txt | cut -d= -f1 | xargs pip install
          }
        fi
        
        # Install the package in development mode
        pip install -e .
    
    - name: Set up environment
      run: |
        # Create required directories
        mkdir -p logs knowledge cache
        
        # Set environment variables
        echo "QDRANT_URL=http://localhost:6333" >> $GITHUB_ENV
        echo "COLLECTION_NAME=mcp-codebase-insight-${{ matrix.python-version }}" >> $GITHUB_ENV
        echo "EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2" >> $GITHUB_ENV
        echo "BUILD_COMMAND=make build" >> $GITHUB_ENV
        echo "TEST_COMMAND=make test" >> $GITHUB_ENV
        echo "MIN_TEST_COVERAGE=${{ github.event.inputs.min_coverage || '80.0' }}" >> $GITHUB_ENV
        echo "MAX_ALLOWED_FAILURES=${{ github.event.inputs.max_failures || '0' }}" >> $GITHUB_ENV
        echo "CRITICAL_MODULES=mcp_codebase_insight.core.vector_store,mcp_codebase_insight.core.knowledge,mcp_codebase_insight.server" >> $GITHUB_ENV
        echo "PYTHON_VERSION=${{ matrix.python-version }}" >> $GITHUB_ENV
    
    - name: Initialize Qdrant collection
      run: |
        echo "Creating Qdrant collection for testing..."
        # Create a basic Python script to initialize the collection
        cat > init_qdrant.py << EOF
        import os
        from qdrant_client import QdrantClient
        from qdrant_client.http import models

        # Connect to Qdrant
        client = QdrantClient(url="http://localhost:6333")
        collection_name = os.environ.get("COLLECTION_NAME", "mcp-codebase-insight-${{ matrix.python-version }}")

        # Check if collection exists
        collections = client.get_collections().collections
        collection_names = [c.name for c in collections]
        
        if collection_name in collection_names:
            print(f"Collection {collection_name} already exists, recreating it...")
            client.delete_collection(collection_name=collection_name)
        
        # Create collection with vector size 384 (for all-MiniLM-L6-v2)
        client.create_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(
                size=384,  # Dimension for all-MiniLM-L6-v2
                distance=models.Distance.COSINE,
            ),
        )
        
        print(f"Successfully created collection {collection_name}")
        EOF
        
        # Run the initialization script
        python init_qdrant.py
        
        # Verify the collection was created
        curl -s http://localhost:6333/collections/$COLLECTION_NAME || (echo "Failed to create Qdrant collection" && exit 1)
        echo "Qdrant collection initialized successfully."

    - name: Create configuration file
      if: ${{ github.event.inputs.config_file != '' }}
      run: |
        cat > ${{ github.event.inputs.config_file }} << EOF
        {
          "success_criteria": {
            "min_test_coverage": ${{ github.event.inputs.min_coverage || '80.0' }},
            "max_allowed_failures": ${{ github.event.inputs.max_failures || '0' }},
            "critical_modules": ["mcp_codebase_insight.core.vector_store", "mcp_codebase_insight.core.knowledge", "mcp_codebase_insight.server"],
            "performance_threshold_ms": 500
          }
        }
        EOF
    
    - name: Run build verification
      id: verify-build
      run: |
        # Make sure the test runner is executable
        chmod +x run_tests.py
        
        # Run tests with standardized runner - use improved async handling
        ./run_tests.py --all --clean --isolated --component --fully-isolated --coverage --html
        
        # Also run integration tests separately
        ./run_tests.py --integration --isolated
        
        # Store exit code for test command
        TEST_EXIT_CODE=$?
        
        CONFIG_ARG=""
        if [ -n "${{ github.event.inputs.config_file }}" ] && [ -f "${{ github.event.inputs.config_file }}" ]; then
          CONFIG_ARG="--config ${{ github.event.inputs.config_file }}"
        fi
        
        # Run the build verification with test results
        python -m scripts.verify_build $CONFIG_ARG --output build-verification-report.json
        VERIFY_EXIT_CODE=$?
        
        # Report failure if either step failed
        if [ $TEST_EXIT_CODE -ne 0 ] || [ $VERIFY_EXIT_CODE -ne 0 ]; then
          echo "::set-output name=failed::true"
        fi
    
    - name: Upload verification report
      uses: actions/upload-artifact@v4
      with:
        name: build-verification-report
        path: build-verification-report.json
    
    - name: Parse verification report
      id: parse-report
      if: always()
      run: |
        if [ -f build-verification-report.json ]; then
          # Extract summary from report
          SUMMARY=$(jq -r '.build_verification_report.summary' build-verification-report.json)
          echo "::set-output name=summary::$SUMMARY"
          
          # Extract status
          STATUS=$(jq -r '.build_verification_report.verification_results.overall_status' build-verification-report.json)
          echo "::set-output name=status::$STATUS"
          
          # Create markdown report
          echo "## Build Verification Report" > report.md
          echo "### Status: $STATUS" >> report.md
          echo "### Summary: $SUMMARY" >> report.md
          
          # Add test results
          echo "### Test Results" >> report.md
          TOTAL=$(jq -r '.build_verification_report.test_summary.total' build-verification-report.json)
          PASSED=$(jq -r '.build_verification_report.test_summary.passed' build-verification-report.json)
          FAILED=$(jq -r '.build_verification_report.test_summary.failed' build-verification-report.json)
          COVERAGE=$(jq -r '.build_verification_report.test_summary.coverage' build-verification-report.json)
          
          echo "- Total Tests: $TOTAL" >> report.md
          echo "- Passed: $PASSED" >> report.md
          echo "- Failed: $FAILED" >> report.md
          echo "- Coverage: $COVERAGE%" >> report.md
          
          # Add failure analysis if available
          if jq -e '.build_verification_report.failure_analysis' build-verification-report.json > /dev/null; then
            echo "### Failures Detected" >> report.md
            jq -r '.build_verification_report.failure_analysis[] | "- " + .description' build-verification-report.json >> report.md
          fi
          
          # Add contextual verification if available
          if jq -e '.build_verification_report.contextual_verification' build-verification-report.json > /dev/null; then
            echo "### Contextual Analysis" >> report.md
            jq -r '.build_verification_report.contextual_verification[] | "#### Module: " + .module + "\n- Failure: " + .failure + "\n- Dependencies: " + (.dependencies | join(", ")) + "\n\n**Potential Causes:**\n" + (.potential_causes | map("- " + .) | join("\n")) + "\n\n**Recommended Actions:**\n" + (.recommended_actions | map("- " + .) | join("\n"))' build-verification-report.json >> report.md
          fi
        else
          echo "::set-output name=summary::Build verification failed - no report generated"
          echo "::set-output name=status::FAILED"
          
          echo "## Build Verification Failed" > report.md
          echo "No report was generated. Check the logs for more information." >> report.md
        fi
        
        cat report.md
    
    - name: Create GitHub check
      uses: LouisBrunner/checks-action@v1.6.2
      if: always()
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        name: Build Verification
        conclusion: ${{ steps.parse-report.outputs.status == 'PASS' && 'success' || 'failure' }}
        output: |
          {
            "title": "Build Verification Results",
            "summary": "${{ steps.parse-report.outputs.summary }}",
            "text": ${{ toJSON(steps.parse-report.outputs.report) }}
          }
    
    - name: Send Slack notification
      uses: 8398a7/action-slack@v3
      if: always()
      with:
        status: ${{ steps.parse-report.outputs.status == 'PASS' && 'success' || 'failure' }}
        fields: repo,message,commit,author,action,eventName,ref,workflow
        text: |
          Build Verification: ${{ steps.parse-report.outputs.status }}
          ${{ steps.parse-report.outputs.summary }}
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      continue-on-error: true
    
    - name: Check verification status
      if: steps.verify-build.outputs.failed == 'true' || steps.parse-report.outputs.status != 'PASS'
      run: |
        echo "Build verification failed!"
        exit 1